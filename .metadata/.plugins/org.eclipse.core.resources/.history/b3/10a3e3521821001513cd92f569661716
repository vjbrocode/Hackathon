package brocode.billing;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Collections;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.poi.hssf.usermodel.HSSFCell;
import org.apache.poi.hssf.usermodel.HSSFRow;
import org.apache.poi.hssf.usermodel.HSSFSheet;
import org.apache.poi.ss.usermodel.Cell;

public class TotalMeterReading {
	public HashMap<Integer, Double> getRates(HSSFSheet sheet) {
		HashMap<Integer, Double> upperRateLimit = new HashMap<Integer,Double>();
		try {
		    HSSFRow row;
		    HSSFCell cell;
	
		    int rows; // No of rows
		    rows = sheet.getPhysicalNumberOfRows();
	
		    int cols = 0; // No of columns
		    int tmp = 0;
	
		    // This trick ensures that we get the data properly even if it doesn't start from first few rows
		    for(int i = 0; i < 10 || i < rows; i++) {
		        row = sheet.getRow(i);
		        if(row != null) {
		            tmp = sheet.getRow(i).getPhysicalNumberOfCells();
		            if(tmp > cols) cols = tmp;
		        }
		    }
	
		    for(int r = 1; r < rows; r++) {
		        row = sheet.getRow(r);
		        if(row != null) {
	                cell = row.getCell(2);
	                if(cell != null && Cell.CELL_TYPE_NUMERIC == cell.getCellType()) {
	                    // Your code here
                		try {
                			upperRateLimit.put((int) row.getCell(1).getNumericCellValue(), row.getCell(2).getNumericCellValue());
                		} catch (Exception e) {
                			upperRateLimit.put(0, row.getCell(2).getNumericCellValue());
                		}
                	}
		        }
		    }
		} catch(Exception ioe) {
		    ioe.printStackTrace();
		}
		return upperRateLimit;
	}
	
	public HashMap<Integer, Integer> getUniqueMeterIDs(HSSFSheet sheet) {
		HashMap<Integer, Integer> meterID = new HashMap<Integer,Integer>();
		try {
			HSSFRow row;
		    HSSFCell cell;
		    int rows; // No of rows
		    rows = sheet.getPhysicalNumberOfRows();
			for(int r = 0; r < rows; r++) {
		        row = sheet.getRow(r);
		        if(row != null) {
	                cell = row.getCell(0);
	                if(cell != null && Cell.CELL_TYPE_NUMERIC == cell.getCellType()) {
		                meterID.put((int) cell.getNumericCellValue(),r);
	                }
		        }
		    }
		} catch (Exception e) {
			System.out.println("Excpetion geting unique meter id " + e);
		}
		return meterID;
	}
	
	public HashMap<String, HashMap<Integer, Integer>> getMinimumAndMaximumReadingByDate(HSSFSheet sheet, HashMap<Integer, Integer> meterID, String dateformat, String fromDate, String toDate) {
		HashMap<Integer, Integer> minReading = new HashMap<Integer,Integer>();
		HashMap<Integer, Integer> maxReading = new HashMap<Integer,Integer>();
		HashMap<String, HashMap<Integer, Integer>> minAndMaxReading = new HashMap<String, HashMap<Integer, Integer>>();
		SimpleDateFormat dateFormat = new SimpleDateFormat(dateformat);
		try {
			dateFormat.parse(fromDate);
			HSSFRow row;
		    HSSFCell cell;
		    int rows; // No of rows
		    rows = sheet.getPhysicalNumberOfRows();
		    for (int id: meterID.keySet()) {
		    	LinkedList<Integer> minList = new LinkedList<Integer>();
		    	LinkedList<Integer> maxList = new LinkedList<Integer>();
				for(int r = 0; r < rows; r++) {
			        row = sheet.getRow(r);
			        if(row != null) {
		                cell = row.getCell(2);
		                if(cell != null && Cell.CELL_TYPE_NUMERIC == cell.getCellType()) {
		        	        if(id == (int)row.getCell(0).getNumericCellValue()) {
		        	        	minList.add((int)cell.getNumericCellValue());
		        	        	maxList.add((int)row.getCell(4).getNumericCellValue());
		        	        }
		                }
		            }
			    }
				minReading.put(id, Collections.min(minList,null));
				maxReading.put(id, Collections.max(maxList,null));
		    }
		} catch (Exception e) {
			System.out.println("getMinimumReadingByDate " + e);
		}
		minAndMaxReading.put("MINReading", minReading);
		minAndMaxReading.put("MAXReading", maxReading);
		return minAndMaxReading;
	}
  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = new Job(conf, "word count");
    job.setJarByClass(TotalMeterReading.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
